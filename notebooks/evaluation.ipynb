{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UI\n",
    "# general imports\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from src import constants\n",
    "\n",
    "\n",
    "# models\n",
    "from src.models import t2i_recsys\n",
    "from src.models.avg_embeddings_model import AvgEmbeddings\n",
    "from src.models.soft_cosine_model import SoftCosine\n",
    "from src.models.knn_model import KNN\n",
    "# from src.models.USE_model import USE_Recsys\n",
    "from src.nlp_util.textacy_util import extract_textrank_from_text\n",
    "\n",
    "# API and UI files\n",
    "from src import api_helper\n",
    "from src import tagging_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from src import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#get training path\n",
    "train_dir = constants.TRAIN_DIR\n",
    "clean_dir = constants.CLEAN_DIR\n",
    "art_prefix = constants.Text_Prefix\n",
    "img_prefix = constants.Media_Prefix\n",
    "#tag types\n",
    "tag_types = ['org', 'place', 'subject','person']\n",
    "# article_summary = pd.read_csv(f'{train_dir}/{art_prefix}summary.csv')\n",
    "article_summary = pd.read_csv(f'{constants.CLEAN_DIR}/article_summary.csv')\n",
    "image_summary =  pd.read_csv(f'{train_dir}/{img_prefix}summary.csv')\n",
    "#preview dir\n",
    "preview_dir = f'{constants.DATA_DIR}/preview'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = article_summary.title[0]\n",
    "body = article_summary.full_text[0]#.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(\"/Users/pwong/Downloads/data_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = article_summary.title[0]\n",
    "body = article_summary.full_text[0]#.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = sample['true_imgs'][0]\n",
    "y_pred = sample['knn'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_true(y):\n",
    "    y = y.replace('[', '').replace(']', '').replace('\"', '').strip().split()\n",
    "    y = [b.replace('\"', '').strip()[1:-1] for b in y]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_pred(y):\n",
    "    y = y.replace('[', '').replace(']', '').replace('\"', '').strip().split(',')\n",
    "    y = [b.replace('\"', '').strip()[1:-1] for b in y]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clean_up_pred(y_pred)\n",
    "y_true = clean_up_true(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = ['a','b']\n",
    "y_pred = ['a', 'c', 'd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2)) \n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    num_true_pos = len(intersection(y_true, y_pred)) # number of true positive\n",
    "    num_true_image = len(y_true) # number of true images used in the article\n",
    "    score = num_true_pos/num_true_image\n",
    "    return score\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    num_true_pos = len(intersection(y_true, y_pred)) # number of true positive\n",
    "    num_pred = len(y_pred) # number of predicted images\n",
    "    score = num_true_pos/num_pred \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_precision(y_true, y_pred):\n",
    "    num_true_image = len(y_true) # number of true images used in the article\n",
    "    num_true_pos = len(intersection(y_true, y_pred[:num_true_image])) # number of true positive\n",
    "    \n",
    "    score = num_true_pos/num_true_image\n",
    "    return score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(r, k, method=0):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "    >>> dcg_at_k(r, 1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 1, method=1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 2)\n",
    "    5.0\n",
    "    >>> dcg_at_k(r, 2, method=1)\n",
    "    4.2618595071429155\n",
    "    >>> dcg_at_k(r, 10)\n",
    "    9.6051177391888114\n",
    "    >>> dcg_at_k(r, 11)\n",
    "    9.6051177391888114\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(y_true, y_pred, k=len(y_true), method=0):\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "    >>> ndcg_at_k(r, 1)\n",
    "    1.0\n",
    "    >>> r = [2, 1, 2, 0]\n",
    "    >>> ndcg_at_k(r, 4)\n",
    "    0.9203032077642922\n",
    "    >>> ndcg_at_k(r, 4, method=1)\n",
    "    0.96519546960144276\n",
    "    >>> ndcg_at_k([0], 1)\n",
    "    0.0\n",
    "    >>> ndcg_at_k([1], 2)\n",
    "    1.0\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Normalized discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = [1* (true_img in y_pred[:len(y_true)]) for true_img in y_true] # 1 if in true images\n",
    "    print(\"r\", r)\n",
    "    dcg_max = dcg_at_k(np.ones(len(y_true)), k = k) # IDCG with ground truth \n",
    "    print('IDCG:', dcg_max)\n",
    "#     dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
